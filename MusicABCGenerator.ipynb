{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T20:47:32.715470Z",
     "iopub.status.busy": "2025-04-13T20:47:32.715180Z",
     "iopub.status.idle": "2025-04-13T21:20:57.401185Z",
     "shell.execute_reply": "2025-04-13T21:20:57.400374Z",
     "shell.execute_reply.started": "2025-04-13T20:47:32.715450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140fabe764dd46659dfc2f4752e909f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b266b62192b0471e9487c6d3cfc79d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89759ba7117e4cbdb660b5e9760fe375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a8a2aa8e73494badec1160149c98c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd71a7bae1154f37b2838fb2bba16a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c03ee9b43d740b78a80916a473694ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4ae7b1ae0e44868708a6b523bcfea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493d9c7442764dfc926822d2df91efb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8842059b30b49bab75d6f7cc65018b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689f2df871f64c30b8511c3bde3af020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685e794ab74040ac93451f9695c338bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling 1000 melodies: 100%|██████████| 1000/1000 [31:53<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saved 1000 labeled melodies to: /kaggle/working/full_labeled_abc.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "import re, json, time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_token\")\n",
    "login(hf_token)\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "def describe_melody_local(melody):\n",
    "    prompt = f\"\"\"This is a melody written in ABC notation:\n",
    "\n",
    "{melody}\n",
    "\n",
    "Please describe the **mood**, **sound type**, and **rhythm** of this melody using one label from each of the following categories:\n",
    "\n",
    "Mood:\n",
    "[happy, sad, emotional, uplifting, tense, melancholy, romantic, angry, calm, dark, energetic, epic, dreamy, nostalgic, hopeful]\n",
    "\n",
    "Sound Type:\n",
    "[solo piano, orchestral, synth heavy, synth pads, bass heavy, melodic lead, percussion-driven, guitar-focused]\n",
    "\n",
    "Rhythm:\n",
    "[no beat, has steady beat, syncopated, irregular, rhythmic pulse]\n",
    "\n",
    "Respond in this format:\n",
    "Mood: <one label>\n",
    "Sound Type: <one label>\n",
    "Rhythm: <one label>\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    if \"Answer:\" in decoded:\n",
    "        return decoded.split(\"Answer:\")[-1].strip()\n",
    "    return decoded.strip()\n",
    "\n",
    "with open(\"/kaggle/input/abc-notation-music-for-rnn/dataabc.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tunes = raw_text.strip().split(\"\\nX:\")\n",
    "tunes = [\"X:\" + t if not t.startswith(\"X:\") else t for t in tunes]\n",
    "\n",
    "def extract_melody(tune):\n",
    "    return \"\\n\".join([\n",
    "        line.strip() for line in tune.splitlines()\n",
    "        if not re.match(r\"^[A-Z]:\", line.strip())\n",
    "    ])\n",
    "\n",
    "melody_only = [extract_melody(t) for t in tunes if len(extract_melody(t)) > 100]\n",
    "\n",
    "labeled_data = []\n",
    "\n",
    "for idx, melody in tqdm(enumerate(melody_only[:1000]), total=1000, desc=\"Labeling 1000 melodies\"):\n",
    "\n",
    "    try:\n",
    "        result = describe_melody_local(melody)\n",
    "        label_dict = {\"melody\": melody}\n",
    "        for line in result.splitlines():\n",
    "            if \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "                label_dict[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
    "        labeled_data.append(label_dict)\n",
    "\n",
    "        time.sleep(0.2) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {idx+1}: {e}\")\n",
    "\n",
    "output_path = \"/kaggle/working/full_labeled_abc.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labeled_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n Saved {len(labeled_data)} labeled melodies to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:20:57.402874Z",
     "iopub.status.busy": "2025-04-13T21:20:57.402603Z",
     "iopub.status.idle": "2025-04-13T21:20:57.411819Z",
     "shell.execute_reply": "2025-04-13T21:20:57.411175Z",
     "shell.execute_reply.started": "2025-04-13T21:20:57.402857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/kaggle/working/full_labeled_abc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "samples = []\n",
    "for item in data:\n",
    "    if all(k in item for k in (\"mood\", \"sound_type\", \"rhythm\", \"melody\")):\n",
    "        prompt = f\"mood: {item['mood']} | sound_type: {item['sound_type']} | rhythm: {item['rhythm']}\"\n",
    "        target = item[\"melody\"]\n",
    "        samples.append({\"input\": prompt, \"output\": target})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:20:57.412628Z",
     "iopub.status.busy": "2025-04-13T21:20:57.412458Z",
     "iopub.status.idle": "2025-04-13T21:20:57.430789Z",
     "shell.execute_reply": "2025-04-13T21:20:57.430196Z",
     "shell.execute_reply.started": "2025-04-13T21:20:57.412614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: mood: uplifting | sound_type: melodic lead | rhythm: has steady beat\n",
      "Target ABC:\n",
      " G3-A (Bcd=e) | f4 (g2dB) | ({d}c3-B) G2-E2 | F4 (D2=E^F) |\n",
      "G3-A (Bcd=e) | f4 d2-f2 | (g2a2 b2).g2 | {b}(a2g2 f2).d2 |\n",
      "(d2{ed}c2) B2B2 | (A2G2 {AG}F2).D2 | (GABc) (d2{ed}c>A) | G2G2 G2z ||\n",
      "G | B2c2 (dcAB) | G2G2 G3G | B2d2 (gfdc) | d2g2 (g3ga) |\n",
      "(bagf) (gd)d>c | (B2AG) F-D.D2 | (GABc) d2d2 | (bgfd) cA.F2 |\n",
      "G2A2 (B2{cB}AG) | A3-G F2-D2 | (GABc) (d2{ed}c>A) | G2G2 G2z2 ||\n",
      "\n",
      "-----\n",
      "\n",
      "Prompt: mood: uplifting | sound_type: melodic lead | rhythm: has steady beat\n",
      "Target ABC:\n",
      " f-g | a3-b g3-a | f4 e3-d | d3-c A3-B | c4 d3-e |\n",
      "d3-c (3(A2G2F2) | G4F2-G2 | A-d3 d3-e | d6 ||\n",
      "A2 | d3-e f3-g | a4 a3-g | a3-b a3-f | g4 g3-g |\n",
      "a3-b a3-g | {e}=f4 e3-c | d3-c A3-G | A6 f-g |\n",
      "a3-b g3-a | f4 e3-d | d3-c A3-B | c4 d3-e |\n",
      "d3-c (3(A2G2F2) | G4 F2-G2 | A-d3 d3-e | d6 ||\n",
      "\n",
      "-----\n",
      "\n",
      "Prompt: mood: epic | sound_type: synth heavy | rhythm: has steady beat\n",
      "Target ABC:\n",
      " B/2-c/2 | d2 d>-c B2 A-B | (GBAG) F2 D-F | (G>AG).F (D>CD).F | G2 G>-A B3 c |\n",
      "(d>ed).c B2 A>-B | (GBAG) F2 D-F | (G>AG).F (D>CD).F | G2 G>A G3 ||\n",
      "D | GABc d2 d=e | f2 =ed c>AF>>c | d>edc B2 AB | GFD=E F2 B-c |\n",
      "(d>ed).c B2 A2 | (GBAG) F2 D-F | (G>AG).F (D>CD).F | G2 G>A G3 ||\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in samples[:3]:\n",
    "    print(\"Prompt:\", sample[\"input\"])\n",
    "    print(\"Target ABC:\\n\", sample[\"output\"])\n",
    "    print(\"-----\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:20:57.431799Z",
     "iopub.status.busy": "2025-04-13T21:20:57.431527Z",
     "iopub.status.idle": "2025-04-13T21:20:59.151416Z",
     "shell.execute_reply": "2025-04-13T21:20:59.150533Z",
     "shell.execute_reply.started": "2025-04-13T21:20:57.431781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0433cb69e5484578b3b732ec2632c88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(samples)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "max_input_len = 128\n",
    "max_target_len = 512\n",
    "\n",
    "def preprocess(example):\n",
    "    model_input = tokenizer(example[\"input\"], padding=\"max_length\", truncation=True, max_length=max_input_len)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"output\"], padding=\"max_length\", truncation=True, max_length=max_target_len)\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=[\"input\", \"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:20:59.153501Z",
     "iopub.status.busy": "2025-04-13T21:20:59.153028Z",
     "iopub.status.idle": "2025-04-13T21:25:32.478958Z",
     "shell.execute_reply": "2025-04-13T21:25:32.478376Z",
     "shell.execute_reply.started": "2025-04-13T21:20:59.153481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 04:30, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.934100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.2377568918863933, metrics={'train_runtime': 271.386, 'train_samples_per_second': 22.109, 'train_steps_per_second': 2.764, 'total_flos': 203012702208000.0, 'train_loss': 1.2377568918863933, 'epoch': 6.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/t5_abc_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=6,\n",
    "    logging_steps=100,                \n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    logging_dir=\"/kaggle/working/logs\",  \n",
    "    report_to=\"none\",                \n",
    "    disable_tqdm=False               \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:25:32.480060Z",
     "iopub.status.busy": "2025-04-13T21:25:32.479859Z",
     "iopub.status.idle": "2025-04-13T21:25:33.013968Z",
     "shell.execute_reply": "2025-04-13T21:25:33.013251Z",
     "shell.execute_reply.started": "2025-04-13T21:25:32.480032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/t5_abc_model/tokenizer_config.json',\n",
       " '/kaggle/working/t5_abc_model/special_tokens_map.json',\n",
       " '/kaggle/working/t5_abc_model/spiece.model',\n",
       " '/kaggle/working/t5_abc_model/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/kaggle/working/t5_abc_model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/t5_abc_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:25:33.015110Z",
     "iopub.status.busy": "2025-04-13T21:25:33.014808Z",
     "iopub.status.idle": "2025-04-13T21:25:33.019818Z",
     "shell.execute_reply": "2025-04-13T21:25:33.019026Z",
     "shell.execute_reply.started": "2025-04-13T21:25:33.015086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from music21 import converter\n",
    "from IPython.display import Audio\n",
    "\n",
    "def abc_to_midi(abc_string, midi_path=\"/kaggle/working/generated.mid\"):\n",
    "    if \"K:\" not in abc_string:\n",
    "        abc_string = \"X:1\\nT:Generated\\nM:4/4\\nK:C\\n\" + abc_string\n",
    "\n",
    "    try:\n",
    "        score = converter.parse(abc_string, format='abc')\n",
    "        score.write('midi', fp=midi_path)\n",
    "        return midi_path\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:25:33.020956Z",
     "iopub.status.busy": "2025-04-13T21:25:33.020715Z",
     "iopub.status.idle": "2025-04-13T21:25:33.120689Z",
     "shell.execute_reply": "2025-04-13T21:25:33.119909Z",
     "shell.execute_reply.started": "2025-04-13T21:25:33.020935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def standardize_abc(abc):\n",
    "    abc = abc.strip()\n",
    "    if \"K:\" not in abc:\n",
    "        abc = \"K:C\\n\" + abc\n",
    "    if \"M:\" not in abc:\n",
    "        abc = \"M:4/4\\n\" + abc\n",
    "    if not abc.startswith(\"X:\"):\n",
    "        abc = \"X:1\\nT:Generated\\n\" + abc\n",
    "    if \"Z:\" not in abc:\n",
    "        abc += \"\\nZ:1\" \n",
    "    return abc\n",
    "\n",
    "samples = []\n",
    "for item in data:\n",
    "    if all(k in item for k in (\"mood\", \"sound_type\", \"rhythm\", \"melody\")):\n",
    "        melody = standardize_abc(item[\"melody\"])\n",
    "        if len(melody.split()) > 20:\n",
    "            prompt = f\"mood: {item['mood']} | sound_type: {item['sound_type']} | rhythm: {item['rhythm']}\"\n",
    "            samples.append({\"input\": prompt, \"output\": melody})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:25:33.121786Z",
     "iopub.status.busy": "2025-04-13T21:25:33.121458Z",
     "iopub.status.idle": "2025-04-13T21:25:34.529268Z",
     "shell.execute_reply": "2025-04-13T21:25:34.528497Z",
     "shell.execute_reply.started": "2025-04-13T21:25:33.121769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cac32061e7418582fbb218d7c01f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "dataset = Dataset.from_list(samples)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def preprocess(example):\n",
    "    model_input = tokenizer(example[\"input\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"output\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=[\"input\", \"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:26:14.646744Z",
     "iopub.status.busy": "2025-04-13T21:26:14.646454Z",
     "iopub.status.idle": "2025-04-13T21:26:14.653519Z",
     "shell.execute_reply": "2025-04-13T21:26:14.652730Z",
     "shell.execute_reply.started": "2025-04-13T21:26:14.646726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_spam(abc):\n",
    "    tokens = abc.split()\n",
    "    if tokens.count(\"f2\") > 15:\n",
    "        return True\n",
    "    if len(set(tokens)) < 6:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_abc_output(abc):\n",
    "    if \"Z:\" in abc:\n",
    "        abc = abc.split(\"Z:\")[0].strip()\n",
    "    if \"K:\" in abc:\n",
    "        abc = \"K:\" + abc.split(\"K:\")[1]\n",
    "    return abc.strip()\n",
    "\n",
    "def generate_abc(prompt_text, max_new_tokens=400, retries=3):\n",
    "    for _ in range(retries):\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_length=80,\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.7,\n",
    "            no_repeat_ngram_size=6,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned = clean_abc_output(result)\n",
    "        if not is_spam(cleaned):\n",
    "            return cleaned\n",
    "    return cleaned  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:26:14.812631Z",
     "iopub.status.busy": "2025-04-13T21:26:14.812343Z",
     "iopub.status.idle": "2025-04-13T21:26:14.817922Z",
     "shell.execute_reply": "2025-04-13T21:26:14.817100Z",
     "shell.execute_reply.started": "2025-04-13T21:26:14.812611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from music21 import converter\n",
    "import re\n",
    "\n",
    "def save_abc_to_midi(abc_string, filename=\"generated.mid\", folder=\"/kaggle/working\"):\n",
    "    abc_clean = re.sub(r\"(?m)^[XTMKLZ]:.*$\", \"\", abc_string).strip()\n",
    "\n",
    "    abc_full = \"X:1\\nT:Generated\\nM:4/4\\nK:C\\n\" + abc_clean + \"\\nZ:1\"\n",
    "\n",
    "    try:\n",
    "        score = converter.parse(abc_full, format='abc')\n",
    "        midi_path = f\"{folder}/{filename}\"\n",
    "        score.write('midi', fp=midi_path)\n",
    "        print(f\"Saved to: {midi_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T21:26:15.060573Z",
     "iopub.status.busy": "2025-04-13T21:26:15.060311Z",
     "iopub.status.idle": "2025-04-13T21:26:16.451900Z",
     "shell.execute_reply": "2025-04-13T21:26:16.451027Z",
     "shell.execute_reply.started": "2025-04-13T21:26:15.060553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /kaggle/working/dreamy_piano.mid\n"
     ]
    }
   ],
   "source": [
    "abc = generate_abc(\"mood: melancholic | sound_type: melodic lead | rhythm: no beat\")\n",
    "save_abc_to_midi(abc, filename=\"dreamy_piano.mid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3035172,
     "sourceId": 5217195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
